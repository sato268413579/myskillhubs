"""
LangChain + LangGraph ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
from typing import Dict, List, Any
from datetime import datetime, date

# LangChain imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

# LangGraph imports
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

# Initialize AI News Research Assistant
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0.2,  # ä½ã„æ¸©åº¦ã§äº‹å®Ÿã«åŸºã¥ãå›ç­”ã‚’é‡è¦–
    convert_system_message_to_human=True
)

# LangGraph State Definition
class TrendResearchState(TypedDict):
    trend: str
    context_info: str
    analysis_result: Dict[str, Any]
    final_result: Dict[str, Any]
    error_message: str

print("âœ… AI News Research Assistant with LangGraph initialized")

# LangGraph Node Functions
def generate_context_node(state: TrendResearchState) -> TrendResearchState:
    """
    LangGraphãƒãƒ¼ãƒ‰: AIã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    """
    trend = state["trend"]
    print(f"ğŸ§  [LangGraph] Generating knowledge-based context for: {trend}")
    
    current_date = date.today()
    current_year = current_date.year
    current_month = current_date.strftime("%Yå¹´%mæœˆ")
    
    context_info = f"""
AIã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ãŸ{trend}ã®èª¿æŸ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:

èª¿æŸ»å¯¾è±¡: {trend}
èª¿æŸ»æ—¥æ™‚: {current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
èª¿æŸ»æ–¹æ³•: LangGraph + AIçŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆ†æ

æ³¨æ„äº‹é …:
- ã“ã®åˆ†æã¯AIã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™
- {current_year}å¹´{current_month}ã®æœ€æ–°æƒ…å ±ã¯å«ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- ã‚ˆã‚Šæ­£ç¢ºãªæœ€æ–°æƒ…å ±ã«ã¤ã„ã¦ã¯ã€å…¬å¼æƒ…å ±æºã®ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™

åˆ†æã®åŸºæº–:
- ä¸€èˆ¬çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹äº‹å®Ÿæƒ…å ±
- æ¥­ç•Œã®åŸºæœ¬çš„ãªå‹•å‘
- æŠ€è¡“çš„ãªèƒŒæ™¯çŸ¥è­˜
- å¸‚å ´ã®ä¸€èˆ¬çš„ãªå‚¾å‘

LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ âœ…
2. AIåˆ†æå®Ÿè¡Œ â†’ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
3. çµæœæœ€çµ‚åŒ– â†’ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—
    """.strip()
    
    state["context_info"] = context_info
    print("âœ… [LangGraph] Context generation completed")
    return state

def analyze_trend_node(state: TrendResearchState) -> TrendResearchState:
    """
    LangGraphãƒãƒ¼ãƒ‰: AIã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ
    """
    trend = state["trend"]
    context_info = state["context_info"]
    print(f"ğŸ¤– [LangGraph] Analyzing trend with AI knowledge for: {trend}")
    
    current_date = datetime.now()
    current_year = current_date.year
    current_month = current_date.strftime("%Yå¹´%mæœˆ")
    
    # AIçŸ¥è­˜ãƒ™ãƒ¼ã‚¹èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    analysis_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=f"""ã‚ãªãŸã¯ä¿¡é ¼æ€§ã®é«˜ã„AIçŸ¥è­˜ãƒ™ãƒ¼ã‚¹èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆLangGraphç‰ˆï¼‰ã§ã™ã€‚
ç¾åœ¨ã®æ—¥ä»˜: {current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}

é‡è¦ãªæŒ‡é‡:
1. **çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ´»ç”¨**: ã‚ãªãŸã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç¢ºå®Ÿãªæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨
2. **éåº¦ãªæ†¶æ¸¬ã®å›é¿**: ä¸ç¢ºå®Ÿãªæƒ…å ±ã«ã¤ã„ã¦ã¯æ˜ç¢ºã«æ³¨è¨˜
3. **æƒ…å ±ã®é®®åº¦æ³¨æ„**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®é™ç•Œã‚’æ˜ç¤º
4. **å®¢è¦³çš„ãªå ±å‘Š**: ä¸­ç«‹çš„ã§å®¢è¦³çš„ãªè¦–ç‚¹ã‚’ç¶­æŒ
5. **æƒ…å ±ã®é™ç•Œæ˜ç¤º**: çŸ¥è­˜ã®ç¯„å›²ã¨é™ç•Œã‚’æ˜ç¢ºã«ç¤ºã™
6. **LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**: æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä¸€éƒ¨ã¨ã—ã¦å®Ÿè¡Œ

å‡ºåŠ›å½¢å¼: å¿…ãšMarkdownå½¢å¼ã§æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
  "trend": "èª¿æŸ»å¯¾è±¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰å",
  "summary": "Markdownå½¢å¼ã®èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆï¼ˆLangGraph + çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ãåˆ†æï¼‰",
  "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5"],
  "insights": [
    "å…·ä½“çš„ãªæ´å¯Ÿ1ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ãï¼‰",
    "å…·ä½“çš„ãªæ´å¯Ÿ2ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ãï¼‰",
    "å…·ä½“çš„ãªæ´å¯Ÿ3ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ãï¼‰"
  ],
  "general_trends": [
    "ä¸€èˆ¬çš„ãªå‹•å‘1",
    "ä¸€èˆ¬çš„ãªå‹•å‘2"
  ],
  "knowledge_sources": [
    "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹æƒ…å ±æºã®ç¨®é¡1",
    "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹æƒ…å ±æºã®ç¨®é¡2"
  ],
  "reliability_note": "ã“ã®åˆ†æã®ä¿¡é ¼æ€§ã¨é™ç•Œã«é–¢ã™ã‚‹æ³¨è¨˜ï¼ˆLangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½¿ç”¨ï¼‰",
  "workflow_info": "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã‚‹æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æãƒ—ãƒ­ã‚»ã‚¹"
}}

Markdownãƒ¬ãƒãƒ¼ãƒˆã®æ§‹é€ ä¾‹:
```markdown
# ğŸ“š {trend}ã®LangGraphçŸ¥è­˜ãƒ™ãƒ¼ã‚¹èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“… èª¿æŸ»æ—¥æ™‚
{current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ç¾åœ¨

## ğŸ”„ LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯LangGraphã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚

## ğŸ” ä¸€èˆ¬çš„ãªå‹•å‘ã¨èƒŒæ™¯
### åŸºæœ¬çš„ãªæ¦‚è¦
- **å®šç¾©**: {trend}ã®åŸºæœ¬çš„ãªå®šç¾©
- **ç‰¹å¾´**: ä¸»è¦ãªç‰¹å¾´ã‚„è¦ç´ 

## ğŸ“Š çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±
### æ¥­ç•Œã®ä¸€èˆ¬çš„ãªçŠ¶æ³
- å¸‚å ´ã®åŸºæœ¬çš„ãªæ§‹é€ 
- ä¸»è¦ãªãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆä¸€èˆ¬çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ï¼‰

## âš ï¸ æƒ…å ±ã®é™ç•Œ
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç‚¹ã§ã®æƒ…å ±
- æœ€æ–°ã®å‹•å‘ã¯å«ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§
- ã‚ˆã‚Šæ­£ç¢ºãªæƒ…å ±ã¯å…¬å¼æƒ…å ±æºã§ç¢ºèªãŒå¿…è¦
```

å¿…ãšJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã€çŸ¥è­˜ã®é™ç•Œã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚"""),
        
        HumanMessage(content=f"""
èª¿æŸ»å¯¾è±¡: {trend}
èª¿æŸ»æ—¥æ™‚: {current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: åˆ†æãƒãƒ¼ãƒ‰å®Ÿè¡Œä¸­

=== èª¿æŸ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ===
{context_info}

LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä¸€éƒ¨ã¨ã—ã¦ã€ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ´»ç”¨ã—ã¦{trend}ã«ã¤ã„ã¦å®¢è¦³çš„ã§äº‹å®Ÿã«åŸºã¥ãèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

èª¿æŸ»ã®é‡ç‚¹:
1. **ç¢ºå®ŸãªçŸ¥è­˜ã®ã¿**ã‚’å ±å‘Š
2. **ä¸€èˆ¬çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹äº‹å®Ÿ**ã‚’ä¸­å¿ƒã«åˆ†æ
3. **æ¥­ç•Œã®åŸºæœ¬çš„ãªå‹•å‘**ã‚’èª¬æ˜
4. **çŸ¥è­˜ã®é™ç•Œ**ã‚’æ˜ç¢ºã«ç¤ºã™
5. **Markdownå½¢å¼**ã§èª­ã¿ã‚„ã™ãæ§‹é€ åŒ–
6. **LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**ã®ä¸€éƒ¨ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ˜è¨˜

ä¸ç¢ºå®Ÿãªæƒ…å ±ã‚„æ¨æ¸¬ã¯é¿ã‘ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç¢ºå®ŸãªçŸ¥è­˜ã®ã¿ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚
æœ€æ–°æƒ…å ±ã®é™ç•Œã«ã¤ã„ã¦ã‚‚æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
        """)
    ])
    
    try:
        response = llm.invoke(analysis_prompt.format_messages())
        analysis_text = response.content.strip()
        
        # JSONã®æŠ½å‡ºã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if analysis_text.startswith("```"):
            analysis_text = analysis_text.strip("`")
            if analysis_text.startswith("json"):
                analysis_text = analysis_text[4:].strip()
        
        analysis = json.loads(analysis_text)
        state["analysis_result"] = analysis
        print("âœ… [LangGraph] AI analysis completed successfully")
        return state
        
    except json.JSONDecodeError as e:
        print(f"âŒ [LangGraph] JSON parsing error: {e}")
        state["error_message"] = f"AIåˆ†æã®JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}"
        state["analysis_result"] = create_fallback_analysis_dict(trend, state["error_message"])
        return state
    except Exception as e:
        print(f"âŒ [LangGraph] AI analysis error: {e}")
        state["error_message"] = f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        state["analysis_result"] = create_fallback_analysis_dict(trend, state["error_message"])
        return state

def finalize_result_node(state: TrendResearchState) -> TrendResearchState:
    """
    LangGraphãƒãƒ¼ãƒ‰: æœ€çµ‚çµæœã‚’ç”Ÿæˆ
    """
    trend = state["trend"]
    analysis_result = state["analysis_result"]
    print(f"ğŸ“‹ [LangGraph] Finalizing research report for: {trend}")
    
    # æœ€çµ‚çµæœã®ç”Ÿæˆ
    final_result = {
        **analysis_result,
        "research_timestamp": datetime.now().isoformat(),
        "research_type": "LangGraph AI Knowledge Research",
        "analysis_method": "LangGraph Workflow + AI Knowledge Base",
        "workflow_steps": [
            "1. Context Generation",
            "2. AI Knowledge Analysis", 
            "3. Result Finalization"
        ]
    }
    
    state["final_result"] = final_result
    print("âœ… [LangGraph] Research report finalized")
    return state

def create_fallback_analysis_dict(trend: str, error_message: str) -> Dict[str, Any]:
    """
    AIåˆ†æãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆè¾æ›¸å½¢å¼ï¼‰
    """
    current_date = datetime.now()
    current_month = current_date.strftime("%Yå¹´%mæœˆ")
    
    fallback_summary = f"""# ğŸ“° {trend}ã®LangGraphèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“… èª¿æŸ»æ—¥æ™‚
{current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ç¾åœ¨

## ğŸ”„ LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ç”Ÿæˆã•ã‚Œã¾ã—ãŸãŒã€åˆ†æä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

## âš ï¸ èª¿æŸ»çŠ¶æ³
{current_month}ç¾åœ¨ã€{trend}ã«é–¢ã™ã‚‹è©³ç´°ãªåˆ†æã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

### ğŸ” èª¿æŸ»ã®åˆ¶é™äº‹é …
- AIåˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ä¸€æ™‚çš„ãªå•é¡Œ
- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼
- LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼

### ğŸ“‹ æ¨å¥¨äº‹é …
1. **å…¬å¼æƒ…å ±æºã®ç¢ºèª**
   - é–¢é€£ä¼æ¥­ã®å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ
   - æ¥­ç•Œå›£ä½“ã®ç™ºè¡¨
   - æ”¿åºœæ©Ÿé–¢ã®å ±å‘Šæ›¸

2. **ä¿¡é ¼ã§ãã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢**
   - ä¸»è¦æ–°èç¤¾ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ
   - æ¥­ç•Œå°‚é–€èªŒ
   - æŠ€è¡“ç³»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ

3. **å†èª¿æŸ»ã®å®Ÿæ–½**
   - æ™‚é–“ã‚’ãŠã„ã¦å†åº¦æ¤œç´¢
   - ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã®æ¤œç´¢
   - è¤‡æ•°ã®æƒ…å ±æºã§ã®ç¢ºèª

## ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
ã‚ˆã‚Šæ­£ç¢ºãªæƒ…å ±ã‚’å¾—ã‚‹ãŸã‚ã«ã€ç›´æ¥çš„ãªæƒ…å ±æºã®ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

*æ³¨: ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æŠ€è¡“çš„ãªå•é¡Œã«ã‚ˆã‚Šé™å®šçš„ãªå†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚*"""

    return {
        "trend": trend,
        "summary": fallback_summary,
        "keywords": [trend, "åˆ†æ", "ã‚¨ãƒ©ãƒ¼"],
        "insights": [
            "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä¸€æ™‚çš„ãªå•é¡ŒãŒç™ºç”Ÿ",
            "è©³ç´°ãªåˆ†æãŒå®Œäº†ã§ããªã„çŠ¶æ³",
            "ç›´æ¥çš„ãªæƒ…å ±æºã®ç¢ºèªãŒæ¨å¥¨ã•ã‚Œã‚‹"
        ],
        "general_trends": [
            f"{current_month}ã®è©³ç´°ãªå‹•å‘åˆ†æã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"
        ],
        "knowledge_sources": [
            "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å‡¦ç†ã«å•é¡ŒãŒç™ºç”Ÿ"
        ],
        "reliability_note": f"LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æŠ€è¡“çš„ãªå•é¡Œã«ã‚ˆã‚Šä¿¡é ¼æ€§ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™: {error_message}",
        "workflow_info": "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "error_info": error_message
    }

# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä½œæˆ
def create_trend_research_workflow():
    """
    LangGraphã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
    """
    workflow = StateGraph(TrendResearchState)
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    workflow.add_node("generate_context", generate_context_node)
    workflow.add_node("analyze_trend", analyze_trend_node)
    workflow.add_node("finalize_result", finalize_result_node)
    
    # ã‚¨ãƒƒã‚¸ã®è¨­å®š
    workflow.set_entry_point("generate_context")
    workflow.add_edge("generate_context", "analyze_trend")
    workflow.add_edge("analyze_trend", "finalize_result")
    workflow.add_edge("finalize_result", END)
    
    return workflow.compile()

# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
trend_research_workflow = create_trend_research_workflow()
print("âœ… LangGraph trend research workflow created")

# å…¬é–‹é–¢æ•°: ãƒ•ãƒ«æ¤œç´¢
def execute_full_search(trend: str) -> Dict[str, Any]:
    """
    LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ•ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        trend: æ¤œç´¢å¯¾è±¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        
    Returns:
        æ¤œç´¢çµæœã®è¾æ›¸
    """
    try:
        print(f"ğŸ¯ [Search] Starting full search for: {trend}")
        
        # LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åˆæœŸçŠ¶æ…‹ã‚’è¨­å®š
        initial_state = TrendResearchState(
            trend=trend,
            context_info="",
            analysis_result={},
            final_result={},
            error_message=""
        )
        
        # LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ
        print("ğŸ”„ [Search] Executing LangGraph workflow...")
        result = trend_research_workflow.invoke(initial_state)
        
        # æœ€çµ‚çµæœã®å–å¾—
        final_result = result["final_result"]
        print(f"ğŸ‰ [Search] Full search completed for: {trend}")
        
        return final_result
        
    except Exception as e:
        print(f"âŒ [Search] Full search error: {e}")
        return create_fallback_analysis_dict(trend, f"ãƒ•ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

# å…¬é–‹é–¢æ•°: ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢
def execute_simple_search(trend: str) -> Dict[str, Any]:
    """
    LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        trend: æ¤œç´¢å¯¾è±¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        
    Returns:
        æ¤œç´¢çµæœã®è¾æ›¸
    """
    try:
        print(f"âš¡ [Search] Starting simple search for: {trend}")
        
        # ç°¡æ˜“ç‰ˆã®åˆæœŸçŠ¶æ…‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        initial_state = TrendResearchState(
            trend=trend,
            context_info=f"ã‚¯ã‚¤ãƒƒã‚¯èª¿æŸ»: {trend}ã®åŸºæœ¬çš„ãªLangGraphçŸ¥è­˜ãƒ™ãƒ¼ã‚¹åˆ†æ",
            analysis_result={},
            final_result={},
            error_message=""
        )
        
        # åˆ†æãƒãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰
        print("ğŸ”„ [Search] Executing simple analysis...")
        state_after_analysis = analyze_trend_node(initial_state)
        final_state = finalize_result_node(state_after_analysis)
        
        final_result = final_state["final_result"]
        final_result["research_type"] = "Quick LangGraph AI Knowledge Research"
        final_result["analysis_method"] = "Quick LangGraph Workflow"
        
        print(f"âš¡ [Search] Simple search completed for: {trend}")
        return final_result
        
    except Exception as e:
        print(f"âŒ [Search] Simple search error: {e}")
        return create_fallback_analysis_dict(trend, f"ã‚·ãƒ³ãƒ—ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

# å…¬é–‹é–¢æ•°: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
def get_search_health_status() -> Dict[str, Any]:
    """
    æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    
    Returns:
        ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è¾æ›¸
    """
    status = {
        "service": "LangGraph AI Knowledge Research Assistant",
        "status": "operational",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "ai_model": "operational" if llm else "unavailable",
            "langgraph_workflow": "operational" if trend_research_workflow else "unavailable",
            "knowledge_base": "operational",
        },
        "analysis_method": "LangGraph Workflow + AI Knowledge Base",
        "workflow_nodes": [
            "generate_context",
            "analyze_trend", 
            "finalize_result"
        ]
    }
    
    # å…¨ä½“çš„ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    if not llm or not trend_research_workflow:
        status["status"] = "degraded"
    
    return status