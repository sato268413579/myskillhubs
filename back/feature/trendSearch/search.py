"""
LangChain + LangGraph ã‚’ä½¿ç”¨ã—ãŸGemini APIãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import requests
from typing import Dict, List, Any
from datetime import datetime, date

# LangChain imports
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

# LangGraph imports
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict

# Initialize Gemini AIï¼ˆè©³ç´°åˆ†æè¨­å®šï¼‰
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0,
    convert_system_message_to_human=True,
    max_output_tokens=8192  # è©³ç´°ãªåˆ†æã®ãŸã‚å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—åŠ 
)

# LangGraph State Definition
class TrendResearchState(TypedDict):
    trend: str
    web_search_results: List[Dict[str, Any]]
    final_result: Dict[str, Any]
    error_message: str

print("âœ… Gemini-based Trend Research Assistant with LangGraph initialized")

# Web Search Functions
def perform_web_search(query: str, num_results: int = 10) -> List[Dict[str, Any]]:
    """
    Webæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ï¼ˆGoogle Custom Search APIä½¿ç”¨ï¼‰
    """
    try:
        # Google Custom Search APIè¨­å®š
        api_key = os.getenv("GOOGLE_SEARCH_API_KEY")
        search_engine_id = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
        
        if not api_key or not search_engine_id:
            print("âš ï¸ Google Search APIèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™")
            return perform_fallback_search(query, num_results)
        
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': api_key,
            'cx': search_engine_id,
            'q': query,
            'num': min(num_results, 10),  # Google API limit
            'dateRestrict': 'm6'  # éå»6ãƒ¶æœˆä»¥å†…ã®çµæœï¼ˆé«˜é€ŸåŒ–ï¼‰
        }
        
        response = requests.get(url, params=params, timeout=5)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’5ç§’ã«çŸ­ç¸®
        response.raise_for_status()
        
        data = response.json()
        results = []
        
        for item in data.get('items', []):
            results.append({
                'title': item.get('title', ''),
                'link': item.get('link', ''),
                'snippet': item.get('snippet', ''),
                'displayLink': item.get('displayLink', ''),
                'formattedUrl': item.get('formattedUrl', '')
            })
        
        print(f"âœ… Webæ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®çµæœã‚’å–å¾—")
        return results
        
    except Exception as e:
        print(f"âŒ Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return perform_fallback_search(query, num_results)

def perform_fallback_search(query: str, num_results: int = 10) -> List[Dict[str, Any]]:
    """
    Webæ¤œç´¢APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨: {query}")
    
    # åŸºæœ¬çš„ãªæ¤œç´¢çµæœã®ãƒ¢ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»–ã®æ¤œç´¢APIã‚„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
    fallback_results = [
        {
            'title': f'{query}ã«é–¢ã™ã‚‹æœ€æ–°æƒ…å ±',
            'link': 'https://example.com/search-unavailable',
            'snippet': f'{query}ã®è©³ç´°ãªæƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã€ç›´æ¥çš„ãªæƒ…å ±æºã®ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚',
            'displayLink': 'example.com',
            'formattedUrl': 'https://example.com/search-unavailable'
        }
    ]
    
    return fallback_results

# LangGraph Node Functions
def web_search_node(state: TrendResearchState) -> TrendResearchState:
    """
    LangGraphãƒãƒ¼ãƒ‰: Webæ¤œç´¢ã‚’å®Ÿè¡Œ
    """
    trend = state["trend"]
    print(f"ğŸŒ [LangGraph Phase 0] Webæ¤œç´¢ã‚’é–‹å§‹: {trend}")
    
    try:
        # 1å›ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã®ã¿å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ï¼‰
        search_query = f"{trend} æœ€æ–°å‹•å‘ 2025"
        
        print(f"ğŸ” æ¤œç´¢ä¸­: {search_query}")
        results = perform_web_search(search_query, 8)  # 8ä»¶ã«åˆ¶é™
        
        state["web_search_results"] = results[:8]  # æœ€å¤§8ä»¶ã«åˆ¶é™
        print(f"âœ… [LangGraph Phase 0] Webæ¤œç´¢å®Œäº†: {len(state['web_search_results'])}ä»¶ã®çµæœ")
        return state
        
    except Exception as e:
        print(f"âŒ [LangGraph Phase 0] Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        state["error_message"] = f"Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"
        state["web_search_results"] = perform_fallback_search(trend, 5)
        return state
def analysis_and_finalize_node(state: TrendResearchState) -> TrendResearchState:
    """
    LangGraphãƒãƒ¼ãƒ‰: Webæ¤œç´¢çµæœã‚’æ´»ç”¨ã—ãŸåŒ…æ‹¬çš„åˆ†æã¨æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    trend = state["trend"]
    web_results = state["web_search_results"]
    print(f"ğŸ¤– [LangGraph Phase 1] åŒ…æ‹¬çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’é–‹å§‹: {trend}")
    
    current_date = datetime.now()
    current_year = current_date.year
    current_month = current_date.strftime("%Yå¹´%mæœˆ")
    
    # Webæ¤œç´¢çµæœã‚’ç°¡æ½”ã«æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆé«˜é€ŸåŒ–ï¼‰
    web_context = ""
    if web_results:
        web_context = "\nã€Webæ¤œç´¢çµæœï¼ˆæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ã€‘\n"
        for i, result in enumerate(web_results[:6], 1):  # æœ€å¤§6ä»¶ã®ã¿ä½¿ç”¨
            web_context += f"{i}. {result['title']}\n"
            web_context += f"   {result['snippet']}\n\n"
    
    # è©³ç´°ãªåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    analysis_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=f"""ã‚ãªãŸã¯{current_month}æ™‚ç‚¹ã§ã®{trend}ã«é–¢ã™ã‚‹èª¿æŸ»ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®Webæ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¦ã€è©³ç´°ã§å…·ä½“çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

{web_context}

å‡ºåŠ›å½¢å¼: ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„
{{
  "detailed_summary": "Markdownå½¢å¼ã®è©³ç´°èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ"
}}

detailed_summaryã«ã¯ä»¥ä¸‹ã®å†…å®¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š

# ğŸ“Š {trend}ã®è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“‹ æ¦‚è¦
- {trend}ã®å®šç¾©ã¨èƒŒæ™¯ã‚’3-4æ–‡ã§èª¬æ˜
- ãªãœä»Šæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã®ã‹å…·ä½“çš„ã«è¨˜è¿°

## ğŸ” ç¾åœ¨ã®çŠ¶æ³ï¼ˆ{current_year}å¹´ï¼‰
- å¸‚å ´è¦æ¨¡ã‚„æ™®åŠçŠ¶æ³ã‚’å…·ä½“çš„ãªæ•°å€¤ã‚„ãƒ‡ãƒ¼ã‚¿ã§èª¬æ˜
- ä¸»è¦ãªãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆä¼æ¥­ã€çµ„ç¹”ã€äººç‰©ï¼‰ã‚’3-5å€‹æŒ™ã’ã¦ã€ãã‚Œãã‚Œã®å½¹å‰²ã‚’èª¬æ˜
- ç¾åœ¨ã®æŠ€è¡“ãƒ¬ãƒ™ãƒ«ã‚„å®Ÿç”¨åŒ–ã®æ®µéšã‚’è©³ã—ãè¨˜è¿°

## ğŸ“ˆ æœ€æ–°å‹•å‘ã¨ãƒˆãƒ¬ãƒ³ãƒ‰
- {current_year}å¹´ã®é‡è¦ãªå‡ºæ¥äº‹ã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’5-7å€‹ã€æ™‚ç³»åˆ—ã§å…·ä½“çš„ã«åˆ—æŒ™
- å„å‹•å‘ã«ã¤ã„ã¦ã€ãªãœé‡è¦ãªã®ã‹ã€ã©ã‚“ãªå½±éŸ¿ãŒã‚ã‚‹ã®ã‹ã‚’èª¬æ˜
- Webæ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæœ€æ–°æƒ…å ±ã‚’å¼•ç”¨

## ğŸ’¡ æŠ€è¡“çš„ãƒ»ãƒ“ã‚¸ãƒã‚¹çš„ãªç‰¹å¾´
- æŠ€è¡“çš„ãªä»•çµ„ã¿ã‚„ç‰¹å¾´ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
- ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã‚„åç›Šæ§‹é€ ã«ã¤ã„ã¦å…·ä½“ä¾‹ã‚’æŒ™ã’ã¦èª¬æ˜
- ä»–ã®é¡ä¼¼æŠ€è¡“ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ã®é•ã„ã‚’æ˜ç¢ºã«

## ğŸŒ å¸‚å ´åˆ†æ
- å¸‚å ´è¦æ¨¡ã®æ¨ç§»ï¼ˆéå»ãƒ»ç¾åœ¨ãƒ»äºˆæ¸¬ï¼‰ã‚’å…·ä½“çš„ãªæ•°å€¤ã§
- æˆé•·ç‡ã‚„å¸‚å ´ã‚·ã‚§ã‚¢ã®ãƒ‡ãƒ¼ã‚¿
- åœ°åŸŸåˆ¥ã®æ™®åŠçŠ¶æ³ã‚„ç‰¹å¾´
- ç«¶åˆçŠ¶æ³ã¨å¸‚å ´ã®æ§‹é€ 

## ğŸ¯ ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æˆ¦ç•¥
- ä¸»è¦ä¼æ¥­ãƒ»çµ„ç¹”ã®å…·ä½“çš„ãªå–ã‚Šçµ„ã¿ã‚’3-5å€‹è©³ã—ãèª¬æ˜
- å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å¼·ã¿ã¨æˆ¦ç•¥ã®é•ã„
- æœ€è¿‘ã®ææºã‚„è²·åãªã©ã®å‹•ã

## ğŸ”® å°†æ¥å±•æœ›ï¼ˆä»Šå¾Œ3-5å¹´ï¼‰
- çŸ­æœŸçš„ãªå±•æœ›ï¼ˆ1-2å¹´ï¼‰ã‚’3-4å€‹å…·ä½“çš„ã«
- ä¸­é•·æœŸçš„ãªå±•æœ›ï¼ˆ3-5å¹´ï¼‰ã‚’3-4å€‹å…·ä½“çš„ã«
- äºˆæƒ³ã•ã‚Œã‚‹å¸‚å ´è¦æ¨¡ã‚„æ™®åŠç‡
- æŠ€è¡“çš„ãªãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã®å¯èƒ½æ€§

## âš ï¸ ãƒªã‚¹ã‚¯ã¨èª²é¡Œ
- æŠ€è¡“çš„ãªèª²é¡Œã‚’3-4å€‹å…·ä½“çš„ã«
- ãƒ“ã‚¸ãƒã‚¹ä¸Šã®èª²é¡Œã‚’3-4å€‹å…·ä½“çš„ã«
- è¦åˆ¶ã‚„æ³•å¾‹é¢ã§ã®æ‡¸å¿µ
- ç¤¾ä¼šçš„ãƒ»å€«ç†çš„ãªå•é¡Œ

## ğŸ’° ãƒ“ã‚¸ãƒã‚¹ãƒãƒ£ãƒ³ã‚¹
- æ–°è¦å‚å…¥ã®æ©Ÿä¼šã‚’3-4å€‹å…·ä½“çš„ã«
- æ—¢å­˜ä¼æ¥­ã®æ´»ç”¨æ–¹æ³•ã‚’3-4å€‹å…·ä½“çš„ã«
- æŠ•è³‡ã‚„å”æ¥­ã®å¯èƒ½æ€§
- æ³¨ç›®ã™ã¹ãå‘¨è¾ºãƒ“ã‚¸ãƒã‚¹

## ğŸ“š å‚è€ƒæƒ…å ±
- Webæ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸé‡è¦ãªæƒ…å ±æº
- é–¢é€£ã™ã‚‹çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚„èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å…·ä½“çš„ãªæ•°å€¤ã€ä¼æ¥­åã€è£½å“åã€äº‹ä¾‹ã‚’å¯èƒ½ãªé™ã‚Šå«ã‚ã¦ãã ã•ã„ã€‚
æŠ½è±¡çš„ãªè¡¨ç¾ã¯é¿ã‘ã€å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å¿…ãšJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""),
        
        HumanMessage(content=f"""
èª¿æŸ»å¯¾è±¡: {trend}
ä¸Šè¨˜ã®Webæ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¦ã€è©³ç´°ã§å…·ä½“çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å…·ä½“ä¾‹ã€æ•°å€¤ã€ä¼æ¥­åãªã©ã‚’å«ã‚ã¦è©³ã—ãè¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        """)
    ])
    
    try:
        response = llm.invoke(analysis_prompt.format_messages())
        analysis_text = response.content.strip()
        
        # JSONã®æŠ½å‡ºã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if analysis_text.startswith("```"):
            analysis_text = analysis_text.strip("`")
            if analysis_text.startswith("json"):
                analysis_text = analysis_text[4:].strip()
        
        analysis_result = json.loads(analysis_text)
        
        # æœ€çµ‚çµæœã‚’ä½œæˆï¼ˆè©³ç´°åˆ†æã®ã¿ï¼‰
        final_result = {
            "detailed_summary": analysis_result.get("detailed_summary", "")
        }
        
        state["final_result"] = final_result
        print("âœ… [LangGraph Phase 1] åŒ…æ‹¬çš„åˆ†æãŒæ­£å¸¸ã«å®Œäº†")
        return state
        
    except json.JSONDecodeError as e:
        print(f"âŒ [LangGraph Phase 1] JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        state["error_message"] = f"åˆ†æã®JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}"
        state["final_result"] = create_fallback_analysis_dict(trend, state["error_message"])
        return state
    except Exception as e:
        print(f"âŒ [LangGraph Phase 1] åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        state["error_message"] = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        state["final_result"] = create_fallback_analysis_dict(trend, state["error_message"])
        return state



def create_fallback_analysis_dict(trend: str, error_message: str) -> Dict[str, Any]:
    """
    åˆ†æãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ
    """
    current_date = datetime.now()
    current_year = current_date.year
    current_month = current_date.strftime("%Yå¹´%mæœˆ")
    
    fallback_summary = f"""# ğŸ“Š {trend}ã®èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“… èª¿æŸ»æ¦‚è¦
- èª¿æŸ»æ—¥æ™‚: {current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
- èª¿æŸ»å¯¾è±¡: {trend}
- èª¿æŸ»æ–¹æ³•: Gemini AI ã«ã‚ˆã‚‹åˆ†æ

## âš ï¸ èª¿æŸ»çŠ¶æ³
{current_month}ç¾åœ¨ã€{trend}ã«é–¢ã™ã‚‹è©³ç´°ãªåˆ†æã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚

### ğŸ” ç™ºç”Ÿã—ãŸå•é¡Œ
- ã‚·ã‚¹ãƒ†ãƒ ã®ä¸€æ™‚çš„ãªå•é¡Œ
- åˆ†æå‡¦ç†ã®ã‚¨ãƒ©ãƒ¼
- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åˆ¶é™

### ğŸ“‹ æ¨å¥¨äº‹é …
1. **å…¬å¼æƒ…å ±æºã®ç¢ºèª**
   - é–¢é€£ä¼æ¥­ã®å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ
   - æ¥­ç•Œå›£ä½“ã®ç™ºè¡¨
   - æ”¿åºœæ©Ÿé–¢ã®å ±å‘Šæ›¸

2. **ä¿¡é ¼ã§ãã‚‹æƒ…å ±æº**
   - ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢
   - æ¥­ç•Œå°‚é–€èªŒ
   - å­¦è¡“ç ”ç©¶æ©Ÿé–¢ã®å ±å‘Š

3. **å†èª¿æŸ»ã®å®Ÿæ–½**
   - æ™‚é–“ã‚’ãŠã„ã¦å†åº¦å®Ÿè¡Œ
   - ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
   - è¤‡æ•°ã®æƒ…å ±æºã§ã®ç¢ºèª

## ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
ã‚ˆã‚Šæ­£ç¢ºãªæƒ…å ±ã‚’å¾—ã‚‹ãŸã‚ã«ã€ç›´æ¥çš„ãªæƒ…å ±æºã®ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

*æ³¨: ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯æŠ€è¡“çš„ãªå•é¡Œã«ã‚ˆã‚Šé™å®šçš„ãªå†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚*"""

    return {
        "detailed_summary": fallback_summary
    }

# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä½œæˆ
def create_trend_research_workflow():
    """
    LangGraphã‚’ä½¿ç”¨ã—ãŸWebæ¤œç´¢+é«˜é€Ÿãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
    """
    workflow = StateGraph(TrendResearchState)
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ ï¼ˆçŠ¶æ…‹ã‚­ãƒ¼ã¨é‡è¤‡ã—ãªã„åå‰ã‚’ä½¿ç”¨ï¼‰
    workflow.add_node("phase0_websearch", web_search_node)
    workflow.add_node("phase1_analysis", analysis_and_finalize_node)
    
    # ã‚¨ãƒƒã‚¸ã®è¨­å®šï¼ˆWebæ¤œç´¢+1æ®µéšåˆ†æãƒ•ãƒ­ãƒ¼ï¼‰
    workflow.set_entry_point("phase0_websearch")
    workflow.add_edge("phase0_websearch", "phase1_analysis")
    workflow.add_edge("phase1_analysis", END)
    
    return workflow.compile()

# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
trend_research_workflow = create_trend_research_workflow()
print("âœ… LangGraph Webæ¤œç´¢ + é«˜é€Ÿãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ")

# å…¬é–‹é–¢æ•°: ãƒ•ãƒ«æ¤œç´¢
def execute_full_search(trend: str) -> Dict[str, Any]:
    """
    LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦Webæ¤œç´¢+é«˜é€Ÿãƒ•ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        trend: æ¤œç´¢å¯¾è±¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        
    Returns:
        æ¤œç´¢çµæœã®è¾æ›¸
    """
    try:
        print(f"ğŸ¯ [Search] Webæ¤œç´¢ + é«˜é€ŸGeminiåˆ†æã‚’é–‹å§‹: {trend}")
        
        # LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åˆæœŸçŠ¶æ…‹ã‚’è¨­å®š
        initial_state = TrendResearchState(
            trend=trend,
            web_search_results=[],
            final_result={},
            error_message=""
        )
        
        # LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ
        print("ğŸ”„ [Search] Webæ¤œç´¢ + é«˜é€ŸLangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œä¸­...")
        print("   ãƒ•ã‚§ãƒ¼ã‚º0: Webæ¤œç´¢ã¨ãƒ‡ãƒ¼ã‚¿åé›†")
        print("   ãƒ•ã‚§ãƒ¼ã‚º1: åŒ…æ‹¬çš„åˆ†æã¨æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        
        result = trend_research_workflow.invoke(initial_state)
        
        # æœ€çµ‚çµæœã®å–å¾—
        final_result = result["final_result"]
        print(f"ğŸ‰ [Search] Webæ¤œç´¢ + é«˜é€ŸGeminiåˆ†æãŒå®Œäº†: {trend}")
        
        return final_result
        
    except Exception as e:
        print(f"âŒ [Search] ãƒ•ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return create_fallback_analysis_dict(trend, f"ãƒ•ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

# å…¬é–‹é–¢æ•°: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
def get_search_health_status() -> Dict[str, Any]:
    """
    æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    
    Returns:
        ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è¾æ›¸
    """
    status = {
        "service": "Webæ¤œç´¢é€£æºGemini AIãƒˆãƒ¬ãƒ³ãƒ‰èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆé«˜é€Ÿç‰ˆï¼‰",
        "status": "ç¨¼åƒä¸­",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "gemini_ai": "ç¨¼åƒä¸­" if llm else "åˆ©ç”¨ä¸å¯",
            "langgraph_workflow": "ç¨¼åƒä¸­" if trend_research_workflow else "åˆ©ç”¨ä¸å¯",
            "web_search": "ç¨¼åƒä¸­" if os.getenv("GOOGLE_SEARCH_API_KEY") else "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰",
        },
        "analysis_method": "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ + Webæ¤œç´¢ + é«˜é€ŸGemini AIåˆ†æ",
        "workflow_nodes": [
            "phase0_websearch",
            "phase1_analysis"
        ],
        "enhancement_features": [
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ Webæ¤œç´¢é€£æº",
            "é«˜é€ŸåŒ…æ‹¬çš„åˆ†æ",
            "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹é‡ç‚¹èª¿æŸ»",
            "åŒ…æ‹¬çš„å¸‚å ´åˆ†æ",
            "ãƒªã‚¹ã‚¯ã¨æ©Ÿä¼šã®è©•ä¾¡"
        ]
    }
    
    # å…¨ä½“çš„ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    if not llm or not trend_research_workflow:
        status["status"] = "æ©Ÿèƒ½åˆ¶é™"
    
    return status